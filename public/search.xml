<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Contrastive Learning</title>
    <url>/2023/03/29/04-23-43/</url>
    <content><![CDATA[<h1 id="对比学习（学习笔记）"><a href="#对比学习（学习笔记）" class="headerlink" title="对比学习（学习笔记）"></a>对比学习（学习笔记）</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>NLP领域的Bert模型，对于这波图像领域的对比学习热潮，是具有启发和推动作用的。我们知道，Bert预训练模型，通过MLM任务的自监督学习，充分挖掘了模型从海量无标注文本中学习通用知识的能力。而图像领域的预训练，往往是有监督的，就是用ImageNet来进行预训练，但是在下游任务中Fine-tuning的效果，跟Bert在NLP下游任务中带来的性能提升，是没法比的。</p>
<span id="more"></span>
<p>因此，对比学习的出现就是要干NLP领域类似Bert预训练的事情，通过自监督学习，不依赖标注数据，要从无标注图像中自己学习知识。图像领域里的自监督可以分为两种类型：生成式自监督学习，判别式自监督学习。对比学习则是典型的判别式自监督学习，相对生成式自监督学习，对比学习的任务难度要低一些。</p>
<p>现有方法基本上可以划分为：基于负例的对比学习方法、基于对比聚类的方法、基于不对称网络结构的方法，以及基于冗余消除损失函数的方法。</p>
]]></content>
      <categories>
        <category>深度学习</category>
        <category>对比学习</category>
      </categories>
      <tags>
        <tag>Contrastive Learning</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>DDPM(Denoising Diffusion Probabilistic Model)公式以及代码解析</title>
    <url>/2023/03/31/15-23-50/</url>
    <content><![CDATA[<h2 id="Diffusion-model之DDPM"><a href="#Diffusion-model之DDPM" class="headerlink" title="Diffusion model之DDPM"></a>Diffusion model之DDPM</h2><blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p>本文理论公式等不做详细推导，只做总结（<del>毕竟是给我自己看的，不是</del>），代码部分会详细解析。 </p>

            <i class="fa fa-quote-right"></i>
          </blockquote>
<hr>
<h3 id="Denoising-Diffusion-Probabilistic-Model"><a href="#Denoising-Diffusion-Probabilistic-Model" class="headerlink" title="Denoising Diffusion Probabilistic Model"></a>Denoising Diffusion Probabilistic Model</h3><h4 id="前向过程："><a href="#前向过程：" class="headerlink" title="前向过程："></a>前向过程：<img src="/2023/03/31/15-23-50/image-20230331160559333.png" alt="image-20230331160559333" style="zoom:50%;"></h4><p>通过从原始数据分布 $\mathbf{x}_0 \sim q(\mathbf{x}_0) $ 不断加高斯噪声，根据马尔可夫链的性质，在T趋于∞的时候可以使其成为一个平稳分布（标准高斯分布） $\mathbf{x}_T \sim \varphi(\mathbf{x})$即 $X \sim N(0,1)$ ，加噪的方差(可以理解为噪声的尺度)由 $\beta_t$控制,调节上一时刻的数据和加入噪声的比例，且随时间步T变化而变化。</p>
<span id="more"></span>
<p>每个时刻加噪后的分布可以表示为 $q(\mathbf{x}_{t}|\mathbf{x}_{t-1}) := \mathcal{N}(\mathbf{x}:\sqrt{1-\beta_t}\mathbf{x}_{t-1}),\beta_tI) $，整个扩散过程可以表示为：$q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right):=\prod_{t=1}^{T} q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right)$。但一步一步加噪是否可以优化，考虑是否可以一步到位，于是便可以根据正态分布的可加性继续推导，用每一个时刻的上一时刻展开表示，其中做一个表示上的代换（ $\alpha_t = 1-\beta_t$ ），最后可以发现规律:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{x}_{t} & =\sqrt{\alpha_{t}} \mathbf{x}_{t-1}+\sqrt{1-\alpha_{t}} \epsilon_{t-1} \\
& =\sqrt{\alpha_{t}}\left(\sqrt{\alpha_{t-1}} \mathbf{x}_{t-2}+\sqrt{1-\alpha_{t-1}} \epsilon_{t-2}\right)+\sqrt{1-\alpha_{t}} \epsilon_{t-1} \\
& =\sqrt{\alpha_{t} \alpha_{t-1}} \mathbf{x}_{t-2}+\sqrt{ {\sqrt{\alpha_{t}-\alpha_{t} \alpha_{t-1}}}^{2}+{\sqrt{1-\alpha_{t}}}^{2}} \bar{\epsilon}_{t-2} \\
& =\sqrt{\alpha_{t} \alpha_{t-1}} \mathbf{x}_{t-2}+\sqrt{1-\alpha_{t} \alpha_{t-1}} \bar{\epsilon}_{t-2} \\
& =\ldots \\
& =\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon
\end{aligned}</script><p>即 $q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)=\mathcal{N}\left(\mathbf{x}_{t} ; \sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0},\left(1-\bar{\alpha}_{t}\right) \mathbf{I}\right)$ ，从而我们可以一步生成任意时刻的图像：</p>
<p><img src="/2023/03/31/15-23-50/image-20230331164416748.png" alt="image-20230331164416748" style="zoom:50%;"></p>
<p>生成 $\mathbf{x}_{t}$的过程我们可以利用<a href="https://zhuanlan.zhihu.com/p/542478018">重参数化</a>技巧得到。</p>
<hr>
<h4 id="逆向过程："><a href="#逆向过程：" class="headerlink" title="逆向过程："></a>逆向过程：<img src="/2023/03/31/15-23-50/image-20230331230650193.png" alt="image-20230331230650193" style="zoom:50%;"></h4><p>逆向是一个去噪的过程，从一个随机高斯噪声开始，一步一步生成原始高清的图片，反向过程也定义为一个马尔可夫过程，要求 $t=0$时刻的数据，即要求 $p_\theta (\mathbf{x}_{0:T}) = p\left(\mathbf{x}_{T}\right) \prod_{t=1}^{T} p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)$，但这里是 $p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)$ 我们是无从得知。</p>
<p>这里我们利用贝叶斯公式进行一个展开：</p>
<script type="math/tex; mode=display">
p_{\theta}\left(\mathbf{x}_{t-1}\mid\mathbf{x}_{t},\mathbf{x}_{0}\right) = q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}, \mathbf{x}_{0}\right)=q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}, \mathbf{x}_{0}\right) \frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{0}\right)}{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)}</script><p>带入正态分布的概率密度函数，展开并配方：</p>
<script type="math/tex; mode=display">
\begin{aligned}
q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}, \mathbf{x}_{0}\right) & =q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}, \mathbf{x}_{0}\right) \frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{0}\right)}{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)} \\
& \propto \exp \left(-\frac{1}{2}\left(\frac{\left(\mathbf{x}_{t}-\sqrt{\alpha_{t}} \mathbf{x}_{t-1}\right)^{2}}{\beta_{t}}+\frac{\left(\mathbf{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0}\right)^{2}}{1-\bar{\alpha}_{t-1}}\right.\right.\left.\left.-\frac{\left(\mathbf{x}_{t}-\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}\right)^{2}}{1-\bar{\alpha}_{t}}\right)\right) \\
&=\exp \left(-\frac{1}{2}\left(\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \mathbf{x}_{t-1}^{2}-\left(\frac{2 \sqrt{\alpha_{t}}}{\beta_{t}} \mathbf{x}_{t}+\frac{2 \sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}} \mathbf{x}_{0}\right) \mathbf{x}_{t-1}\right.\right.\left.\left.+C\left(\mathbf{x}_{t}, \mathbf{x}_{0}\right)\right)\right)

\end{aligned}</script><p>配方后可以用一个新的分布表示:         </p>
<script type="math/tex; mode=display">
q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}, \mathbf{x}_{0}\right) ～ \mathcal{N}(\mathbf{x}_{t-1};\tilde{\mathbf{\mu}} (\mathbf{x}_t,\mathbf{x}_{0}),\tilde{\beta_t}\mathbf{I})</script><p>其中： </p>
<script type="math/tex; mode=display">
\begin{gather}
\tilde{\mathbf{\mu}} (\mathbf{x}_t,\mathbf{x}_{0})=\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \mathbf{x}_t+\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathbf{x}_0   \\
\tilde{\beta}_t=1 /\left(\frac{\alpha_t}{\beta_t}+\frac{1}{1-\bar{\alpha}_{t-1}}\right)=1 /\left(\frac{\alpha_t-\bar{\alpha}_t+\beta_t}{\beta_t\left(1-\bar{\alpha}_{t-1}\right)}\right)=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \cdot \beta_t
\end{gather}</script><p>其中方差可以看出是一个定值，而均值和 $\mathbf{x}_t$、$\mathbf{x}_0$有关，$\mathbf{x}_t$是已知输入，$\mathbf{x}_0$可以由前向过程一步加噪的式子代入化简。</p>
<script type="math/tex; mode=display">
\begin{gather}
\mathbf{x}_t=\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon} \\
\mathbf{x}_0 = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}\right) \\
\boldsymbol{\mu}_\theta\left(\mathbf{x}_t, t\right)=\tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t, \frac{1}{\sqrt{\bar{\alpha}_t}}\left(\mathbf{x}_t-\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}_\theta\left(\mathbf{x}_t\right)\right)\right)=\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta\left(\mathbf{x}_t, t\right)\right)
\end{gather}</script><p>这里我们如果我们有这个均值和方差，便可以根据这两个参数从正态分布中采样出一个数据，即得到 $\mathbf{x}_{t-1}$。但是我们没有噪声 $\boldsymbol{\epsilon}$的信息，现在可以考虑用一个神经网络$\boldsymbol{\epsilon}_\theta$ 去学习这个$\boldsymbol{\epsilon}$的信息，也可以直接学习均值的信息（但论文实验表明直接预测均值效果不好）。于是考虑用一个conditional的Unet去预测，$\boldsymbol{\epsilon}_\theta = U net(\mathbf{x}_t,t)$，之后便可以从$\mathbf{x}_t$预测得到$\boldsymbol{\epsilon}$，进而Sample出$\mathbf{x}_{t-1}$，直到$\mathbf{x}_0$</p>
<script type="math/tex; mode=display">
\mathbf{x}_{t-1}=\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta\left(\mathbf{x}_t, t\right)\right)+\sigma_t \mathbf{z}</script><hr>
<h4 id="训练过程："><a href="#训练过程：" class="headerlink" title="训练过程："></a>训练过程：</h4><p><img src="/2023/03/31/15-23-50/image-20230401005008193.png" alt="image-20230401005008193" style="zoom:50%;"></p>
<ol>
<li>首先从我们的数据分布中sample出一个batch的数据 —&gt; B x C x H x W</li>
<li>然后对整个batch分配从均匀分布中随机采样出的t —&gt; B x 1 （每个样本的t不是相同的）</li>
<li>从标准正态分布中采样一个噪音，根据噪音和 $\mathbf{x}_{0}$利用公式直接生成 $\mathbf{x}_{t}$</li>
<li>根据生成的$\mathbf{x}_{t}$和$t$利用Unet预测一个噪音，和初始sample的噪声计算MSE loss</li>
<li>根据梯度优化Unet参数。</li>
</ol>
<h4 id="采样过程："><a href="#采样过程：" class="headerlink" title="采样过程："></a>采样过程：</h4><p><img src="/2023/03/31/15-23-50/image-20230401005051290.png" alt="image-20230401005051290" style="zoom:50%;"></p>
<ol>
<li>从标准正态分布中采样一个噪声作为 $\mathbf{x}_{T}$</li>
<li>执行T步循环：输入当前t和 $\mathbf{x}_{t}$，得到预测的 $\boldsymbol{\epsilon}_\theta$，根据公式计算均值和方差（DDPM里是固定的），然后从新的分布中利用重参数技巧重新采样获得上一时刻 $\mathbf{x}_{t-1}$。直到 $t=1$时结束。</li>
</ol>
<h3 id="代码解析："><a href="#代码解析：" class="headerlink" title="代码解析："></a>代码解析：</h3><blockquote>
<p>这里对Lucidrains大佬复现的Pytorch版DDPM代码进行分析。（<del>简直是优雅</del>）</p>
</blockquote>
<ul>
<li><a href="https://github.com/lucidrains/denoising-diffusion-pytorch">代码参考https://github.com/lucidrains/denoising-diffusion-pytorch</a></li>
</ul>
<h4 id="代码分析顺序："><a href="#代码分析顺序：" class="headerlink" title="代码分析顺序："></a>代码分析顺序：</h4><ul>
<li><h5 id="模型结构-—-gt-训练-—-gt-采样"><a href="#模型结构-—-gt-训练-—-gt-采样" class="headerlink" title="模型结构 $—&gt;$ 训练 $—&gt;$ 采样"></a>模型结构 $—&gt;$ 训练 $—&gt;$ 采样</h5></li>
</ul>
<h4 id="模型结构："><a href="#模型结构：" class="headerlink" title="模型结构："></a>模型结构：</h4><ol>
<li><h6 id="Unet：用于预测noise，是带有Conditional的Unet-x-t-，通过self-attention模块把Position-Embedding后的时间步-mathbf-T-融合到Unet中，从而能更好对于不同时刻的-mathbf-X-T-进行噪声的预测。训练后模型保存的就是这块的参数。"><a href="#Unet：用于预测noise，是带有Conditional的Unet-x-t-，通过self-attention模块把Position-Embedding后的时间步-mathbf-T-融合到Unet中，从而能更好对于不同时刻的-mathbf-X-T-进行噪声的预测。训练后模型保存的就是这块的参数。" class="headerlink" title="Unet：用于预测noise，是带有Conditional的Unet(x,t)，通过self-attention模块把Position Embedding后的时间步$\mathbf{T}$融合到Unet中，从而能更好对于不同时刻的 $\mathbf{X}_T$进行噪声的预测。训练后模型保存的就是这块的参数。"></a>Unet：用于预测<code>noise</code>，是带有Conditional的Unet(x,t)，通过self-attention模块把Position Embedding后的时间步$\mathbf{T}$融合到Unet中，从而能更好对于不同时刻的 $\mathbf{X}_T$进行噪声的预测。训练后模型保存的就是这块的参数。</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Unet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        dim,</span></span><br><span class="line"><span class="params">        init_dim = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        out_dim = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        dim_mults=(<span class="params"><span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span></span>),				<span class="comment">##对应Unet下采样4个阶段通道数的变换，数值对应变化的倍数</span></span></span><br><span class="line"><span class="params">        channels = <span class="number">3</span>,								  <span class="comment">##输入图像的通道数</span></span></span><br><span class="line"><span class="params">        self_condition = <span class="literal">False</span>,				<span class="comment">##控制Unet中是否加入自注意力机制。		</span></span></span><br><span class="line"><span class="params">        resnet_block_groups = <span class="number">8</span>,</span></span><br><span class="line"><span class="params">        learned_variance = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        learned_sinusoidal_cond = <span class="literal">False</span>,						</span></span><br><span class="line"><span class="params">        random_fourier_features = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        learned_sinusoidal_dim = <span class="number">16</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># determine dimensions</span></span><br><span class="line"></span><br><span class="line">        self.channels = channels</span><br><span class="line">        self.self_condition = self_condition</span><br><span class="line">        input_channels = channels * (<span class="number">2</span> <span class="keyword">if</span> self_condition <span class="keyword">else</span> <span class="number">1</span>)			</span><br><span class="line"></span><br><span class="line">        init_dim = default(init_dim, dim)  <span class="comment">##init_dim有定义则设置为init_dim，无就默认dim</span></span><br><span class="line">        self.init_conv = nn.Conv2d(input_channels, init_dim, <span class="number">7</span>, padding = <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        dims = [init_dim, *<span class="built_in">map</span>(<span class="keyword">lambda</span> m: dim * m, dim_mults)]</span><br><span class="line">        in_out = <span class="built_in">list</span>(<span class="built_in">zip</span>(dims[:-<span class="number">1</span>], dims[<span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line">        block_klass = partial(ResnetBlock, groups = resnet_block_groups)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># time embeddings</span></span><br><span class="line"></span><br><span class="line">        time_dim = dim * <span class="number">4</span></span><br><span class="line"></span><br><span class="line">        self.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond <span class="keyword">or</span> random_fourier_features</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.random_or_learned_sinusoidal_cond:</span><br><span class="line">            sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)</span><br><span class="line">            fourier_dim = learned_sinusoidal_dim + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sinu_pos_emb = SinusoidalPosEmb(dim)		<span class="comment">##正弦位置编码</span></span><br><span class="line">            fourier_dim = dim</span><br><span class="line"></span><br><span class="line">        self.time_mlp = nn.Sequential(		<span class="comment">##对步长进行一个编码和转换的MLP</span></span><br><span class="line">            sinu_pos_emb,</span><br><span class="line">            nn.Linear(fourier_dim, time_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Linear(time_dim, time_dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># layers</span></span><br><span class="line"></span><br><span class="line">        self.downs = nn.ModuleList([])</span><br><span class="line">        self.ups = nn.ModuleList([])</span><br><span class="line">        num_resolutions = <span class="built_in">len</span>(in_out)</span><br><span class="line">	<span class="comment">##堆叠下采样残差块</span></span><br><span class="line">        <span class="keyword">for</span> ind, (dim_in, dim_out) <span class="keyword">in</span> <span class="built_in">enumerate</span>(in_out):</span><br><span class="line">            is_last = ind &gt;= (num_resolutions - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            self.downs.append(nn.ModuleList([</span><br><span class="line">                block_klass(dim_in, dim_in, time_emb_dim = time_dim),</span><br><span class="line">                block_klass(dim_in, dim_in, time_emb_dim = time_dim),</span><br><span class="line">                Residual(PreNorm(dim_in, LinearAttention(dim_in))),</span><br><span class="line">                Downsample(dim_in, dim_out) <span class="keyword">if</span> <span class="keyword">not</span> is_last <span class="keyword">else</span> nn.Conv2d(dim_in, dim_out, <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">        mid_dim = dims[-<span class="number">1</span>]</span><br><span class="line">        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)</span><br><span class="line">        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))</span><br><span class="line">        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)</span><br><span class="line">	<span class="comment">##堆叠下采样残差块</span></span><br><span class="line">        <span class="keyword">for</span> ind, (dim_in, dim_out) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">reversed</span>(in_out)):</span><br><span class="line">            is_last = ind == (<span class="built_in">len</span>(in_out) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            self.ups.append(nn.ModuleList([</span><br><span class="line">                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),</span><br><span class="line">                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),</span><br><span class="line">                Residual(PreNorm(dim_out, LinearAttention(dim_out))),</span><br><span class="line">                Upsample(dim_out, dim_in) <span class="keyword">if</span> <span class="keyword">not</span> is_last <span class="keyword">else</span>  nn.Conv2d(dim_out, dim_in, <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">        default_out_dim = channels * (<span class="number">1</span> <span class="keyword">if</span> <span class="keyword">not</span> learned_variance <span class="keyword">else</span> <span class="number">2</span>)</span><br><span class="line">        self.out_dim = default(out_dim, default_out_dim)</span><br><span class="line"></span><br><span class="line">        self.final_res_block = block_klass(dim * <span class="number">2</span>, dim, time_emb_dim = time_dim)</span><br><span class="line">        self.final_conv = nn.Conv2d(dim, self.out_dim, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li>GassianDiffusion：将整个diffusion process和reverse diffusion集成在一起的类，载入了Unet模型。可以进行任意时间步的扩散，以及单步采样和整个时间步长的采样循环。同时提供了训练时计算Loss的过程，整个<code>forward</code>过程返回的就是训练的Loss。</li>
</ol>
<h4 id="训练过程：-1"><a href="#训练过程：-1" class="headerlink" title="训练过程："></a>训练过程：</h4><p>看decoder后分类变不变，或者latent diffusion后分类变不变</p>
<p>ldm换l1，unet通道倍数1，2，2，2</p>
<p>ae的训练encode，只在encoder里除10.</p>
]]></content>
      <categories>
        <category>深度学习</category>
        <category>Diffusion model</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Diffusion model</tag>
        <tag>代码讲解</tag>
      </tags>
  </entry>
  <entry>
    <title>衡量相似性的方法</title>
    <url>/2023/03/30/00-46-40/</url>
    <content><![CDATA[<blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p>保持距离！ </p>

            <i class="fa fa-quote-right"></i>
          </blockquote>
<h2 id="相似性"><a href="#相似性" class="headerlink" title="相似性"></a>相似性</h2><p>对于深度学习的各种任务，我们常常需要最小化模型输出与真实标签之间的距离，我们往往采用均方误差，交叉熵等作为损失函数，换句话说，不管是拉近它们分布之间的距离，还是什么，都是在使它们这两个张量更为相似。因此我们任何能衡量两组数据的相似性的方法都可以作为损失函数。这里总结一下各种计算相似度的方法。</p>
<hr>
<h3 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h3><blockquote>
<p><strong>余弦相似性</strong>通过测量两个<a href="https://baike.baidu.com/item/向量?fromModule=lemma_inlink">向量</a>的夹角的<a href="https://baike.baidu.com/item/余弦?fromModule=lemma_inlink">余弦</a>值来度量它们之间的相似性。0度角的余弦值是1，而其他任何角度的余弦值都不大于1；并且其最小值是-1。从而两个向量之间的角度的余弦值确定两个向量是否大致指向相同的方向。两个向量有相同的指向时，余弦相似度的值为1；两个向量夹角为90°时，余弦相似度的值为0；两个向量指向完全相反的方向时，余弦相似度的值为-1。这结果是与向量的长度无关的，仅仅与向量的指向方向相关。</p>
</blockquote>
<span id="more"></span>
<h4 id="计算方式："><a href="#计算方式：" class="headerlink" title="计算方式："></a>计算方式：</h4><p>两个向量间的余弦值可以通过使用欧几里得点积公式求出：</p>
<p><img src="/2023/03/30/00-46-40/image-20230330005905251.png" alt="image-20230330005905251" style="zoom:50%;"></p>
<p>给定两个属性向量，<em>A</em>和<em>B</em>，其余弦相似性<em>θ</em>由点积和向量长度计算出：</p>
<p><img src="/2023/03/30/00-46-40/image-20230330005954209.png" alt="image-20230330005954209" style="zoom:50%;"></p>
<h5 id="Pytorch代码："><a href="#Pytorch代码：" class="headerlink" title="Pytorch代码："></a>Pytorch代码：</h5><p><img src="/2023/03/30/00-46-40/image-20230330193750294.png" alt="image-20230330193750294" style="zoom:50%;"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#given x1 and x2</span></span><br><span class="line">cos = torch.nn.CosineSimilarity(dim=<span class="number">1</span>,eps=<span class="number">1e-8</span>) <span class="comment">#eps防止分母除到0</span></span><br><span class="line">output = cos(x1,x2)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="感知相似性（Perceptual-Similarity）"><a href="#感知相似性（Perceptual-Similarity）" class="headerlink" title="感知相似性（Perceptual Similarity）"></a><strong>感知相似性（Perceptual Similarity）</strong></h3><blockquote>
<p>LPIPS：来源于CVPR2018的一篇论文《The Unreasonable Effectiveness of Deep Features as a Perceptual Metric》，该度量标准学习生成图像到Ground Truth的反向映射强制生成器学习从假图像中重构真实图像的反向映射，并优先处理它们之间的感知相似度，更符合人类的感知，可以用于衡量两个图片之间的相似度。</p>
</blockquote>
<h4 id="计算方法："><a href="#计算方法：" class="headerlink" title="计算方法："></a>计算方法：</h4><p><img src="/2023/03/30/00-46-40/image-20230330202419413.png" alt="image-20230330202419413" style="zoom:50%;"></p>
<p><img src="/2023/03/30/00-46-40/image-20230330202503729.png" alt="image-20230330202503729" style="zoom:30%;"></p>
<p>通过对各通道加和，空间平均每一层网络输出的特征图之间的距离。然后将得到的两个距离输入到评分网络中。$W_l$是与通道数同维度的可训练权重参数。其中$F$表示预训练特征提取模型，如Vgg，Alex等。</p>
<h5 id="Pytorch代码：-1"><a href="#Pytorch代码：-1" class="headerlink" title="Pytorch代码："></a>Pytorch代码：</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!pip install lpips</span></span><br><span class="line"><span class="keyword">import</span> lpips</span><br><span class="line">lpips_fn = lpips.LPIPS(net=<span class="string">&#x27;vgg&#x27;</span>)</span><br><span class="line">output = lpips_fn(x1,x2)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Next设置背景与文本框的透明度</title>
    <url>/2023/03/27/17-53-08/</url>
    <content><![CDATA[<h2 id="设置Hexo背景"><a href="#设置Hexo背景" class="headerlink" title="设置Hexo背景"></a>设置Hexo背景</h2><p>找到next主题下面的source/css/_schemes/Mist/index.styl，添加如下代码：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">body &#123; </span><br><span class="line">    background:url(/uploads/1.png);</span><br><span class="line">    background-repeat: no-repeat;</span><br><span class="line">    background-attachment:fixed; //不重复</span><br><span class="line">    background-size: cover;      //填充</span><br><span class="line">    background-position:100% 100%;</span><br><span class="line">&#125;       </span><br></pre></td></tr></table></figure></p>
<h2 id="设置文本框的透明度"><a href="#设置文本框的透明度" class="headerlink" title="设置文本框的透明度"></a>设置文本框的透明度</h2><p>可以再添加如下代码：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.post-block &#123;</span><br><span class="line">  background: rgba(255,255,255,.85) none repeat scroll !important;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><em>rgba的第四个参数是控制透明度，其余控制文章背景的颜色。</em></p>
<p>其余美化参考这篇<a href="https://blog.csdn.net/lyshark_csdn/article/details/124939308?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-124939308-blog-93797964.235^v27^pc_relevant_default&amp;spm=1001.2101.3001.4242.1&amp;utm_relevant_index=3">博客</a></p>
]]></content>
      <categories>
        <category>Hexo配置</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>在Typora里插入图片(并在Hexo本地能显示)</title>
    <url>/2023/03/28/20-45-40/</url>
    <content><![CDATA[<h2 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h2><p><img src="/2023/03/28/20-45-40/image-20230328210953868.png" alt="image-20230328210953868"></p>
<span id="more"></span>
<h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><h3 id="先下载Typora并配置偏好设置"><a href="#先下载Typora并配置偏好设置" class="headerlink" title="先下载Typora并配置偏好设置"></a>先下载Typora并配置偏好设置</h3><p><img src="/2023/03/28/20-45-40/image-20230328211235474.png" alt="image-20230328211235474" style="zoom:50%;"></p>
<h3 id="并且在博客根目录下安装-hexo-asset-image"><a href="#并且在博客根目录下安装-hexo-asset-image" class="headerlink" title="并且在博客根目录下安装 hexo-asset-image"></a>并且在博客根目录下安装 <code>hexo-asset-image</code></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install https://github.com/CodeFalling/hexo-asset-image --save</span><br></pre></td></tr></table></figure>
<p>修改 <code>_config.yml</code>中 <code>post_asset_folder</code>设置为<code>true</code></p>
<p>此后每 <code>hexo new page</code>都会产生一个同名的文件夹，用于存放图片。</p>
<h3 id="安装插件-hexo-renderer-marked"><a href="#安装插件-hexo-renderer-marked" class="headerlink" title="安装插件 hexo-renderer-marked"></a>安装插件 <code>hexo-renderer-marked</code></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-renderer-marked --save</span><br></pre></td></tr></table></figure>
<p>并且在<code>_config.yml</code>中加入如下设置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">marked:</span><br><span class="line">  prependRoot: true</span><br><span class="line">  postAsset: true</span><br></pre></td></tr></table></figure>
<h3 id="大功告成"><a href="#大功告成" class="headerlink" title="大功告成"></a>大功告成</h3><p>之后在哪复制的或者截图的就可以直接粘贴进Typora了</p>
]]></content>
      <categories>
        <category>Hexo配置</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/03/27/15-38-50/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>hexo-bug</title>
    <url>/2023/03/31/22-41-53/</url>
    <content><![CDATA[<h2 id="Hexo遇到的BUG"><a href="#Hexo遇到的BUG" class="headerlink" title="Hexo遇到的BUG"></a>Hexo遇到的BUG</h2><h4 id="编辑数学公式时："><a href="#编辑数学公式时：" class="headerlink" title="编辑数学公式时："></a>编辑数学公式时：</h4><p>使用Latex表示公式时连续使用 <code>&#123; &#123;</code>会报错，且无法渲染，需要在括号之间加入空格完美解决。<del>折磨了半天，竟然这么简单的解决了，之前还查了什么导致和markdown冲突，需要改解析的规则</del></p>
]]></content>
  </entry>
</search>
